{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "\n",
    "from econml.sklearn_extensions.linear_model import StatsModelsLinearRegression\n",
    "from econml.utilities import _safe_norm_ppf\n",
    "\n",
    "\n",
    "def dr_sim(model_t, model_y, n_folds, Y, T, X, min_t=0.01):\n",
    "    \"\"\"\n",
    "    Doubly-robust learner\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model_t : object\n",
    "        A classifier for the treatment.\n",
    "    model_y : object\n",
    "        A regressor for the outcome.\n",
    "    n_folds : int\n",
    "        The number of folds to use in cross-validation.\n",
    "    Y : array-like\n",
    "        The outcome variable.\n",
    "    T : array-like\n",
    "        The treatment variable.\n",
    "    X : array-like\n",
    "        The covariates.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    theta : array-like\n",
    "        The estimated treatment effect per treatment.\n",
    "    sigma : array-like\n",
    "        The estimated variance of the outcome residuals per treatment.\n",
    "    nu : array-like\n",
    "        The estimated variance of the treatment per treatment.\n",
    "    cov : array-like\n",
    "        The covariance matrix of the estimated parameters, of shape (n_treatments, 3, 3).\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize KFold cross-validation\n",
    "    kf = StratifiedKFold(n_splits=n_folds, shuffle=True)\n",
    "    T_ohe = OneHotEncoder(sparse_output=False, drop='first').fit_transform(T.reshape(-1, 1))\n",
    "\n",
    "    # Initialize arrays to hold predictions\n",
    "    y_pred = np.zeros((Y.shape[0], T_ohe.shape[1]+1)) # include prediction for T=0\n",
    "    t_pred = np.zeros((T_ohe.shape[0], T_ohe.shape[1]+1)) # include prediction for T=0\n",
    "    alpha = np.zeros_like(T_ohe)\n",
    "\n",
    "    # one value of theta, sigma, nu for each non-control treatment\n",
    "    theta = np.zeros(T_ohe.shape[1])\n",
    "    sigma = np.zeros(T_ohe.shape[1])\n",
    "    nu = np.zeros(T_ohe.shape[1])\n",
    "\n",
    "    # one theta, sigma, nu covariance matrix for each non-control treatment\n",
    "    cov = np.zeros((T_ohe.shape[1], 3, 3))\n",
    "\n",
    "    # Cross-validation loop\n",
    "    for train_index, test_index in kf.split(X,T):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "        T_train = T[train_index]\n",
    "        To_train, To_test = T_ohe[train_index], T_ohe[test_index]\n",
    "     \n",
    "        # Fit the treatment model\n",
    "        t_pred[test_index] = model_t.fit(X_train, T_train).predict_proba(X_test)\n",
    "\n",
    "        t_pred[test_index] = np.clip(t_pred[test_index], min_t, 1-min_t)  # avoid division by zero\n",
    "        t_pred[test_index] = t_pred[test_index] / np.sum(t_pred[test_index], axis=1, keepdims=True)  # normalize to sum to 1\n",
    "        \n",
    "        # Fit the outcome model\n",
    "        model_y.fit(np.hstack([X_train, To_train]), Y_train)\n",
    "        T_hypo = np.zeros_like(To_test)\n",
    "        y_pred[test_index,0] += model_y.predict(np.hstack([X_test, T_hypo]))\n",
    "        for i in range(To_train.shape[1]):\n",
    "            T_hypo = np.zeros_like(To_test)\n",
    "            T_hypo[:,i] = 1\n",
    "            y_pred[test_index,i+1] += model_y.predict(np.hstack([X_test, T_hypo]))\n",
    "\n",
    "    # ATE, sigma^2, and nu^2\n",
    "    for i in range(T_ohe.shape[1]):\n",
    "        theta_score = y_pred[:,i+1] - y_pred[:,0] + (Y-y_pred[:,i+1]) * (T_ohe[:,i] == 1)/t_pred[:,i+1] - (Y-y_pred[:,0]) * (np.all(T_ohe==0, axis=1)/t_pred[:,0])\n",
    "        sigma_score = (Y-np.choose(T, y_pred.T))**2 # exclude rows with other treatments\n",
    "        alpha[:,i] = (T_ohe[:,i] == 1)/t_pred[:,i+1] - (np.all(T_ohe==0, axis=1))/t_pred[:,0]\n",
    "        nu_score = 2*(1/t_pred[:,i+1]+1/t_pred[:,0])-alpha[:,i]**2\n",
    "        theta[i] = np.mean(theta_score)\n",
    "        sigma[i] = np.mean(sigma_score)\n",
    "        nu[i] = np.mean(nu_score)\n",
    "        scores = np.stack([theta_score-theta[i], sigma_score-sigma[i], nu_score-nu[i]], axis=1)\n",
    "        cov[i,:,:] = (scores.T @ scores / len(scores)) / len(scores)\n",
    "\n",
    "    return theta, sigma, nu, cov\n",
    "\n",
    "def sensitivity_interval(theta, sigma, nu, cov, alpha, c_y, c_t, rho):\n",
    "    \"\"\"\n",
    "    Calculate the sensitivity interval for a doubly-robust learner.\n",
    "    \"\"\"\n",
    "    C = np.abs(rho) * np.sqrt(c_y) * np.sqrt(c_t/(1-c_t))/2\n",
    "    ests = np.array([theta, sigma, nu])\n",
    "    \n",
    "    coefs_p = np.array([1, C*np.sqrt(nu/sigma), C*np.sqrt(sigma/nu)])\n",
    "    coefs_n = np.array([1, -C*np.sqrt(nu/sigma), -C*np.sqrt(sigma/nu)])\n",
    "    # One dimensional normal distribution:\n",
    "    sigma_p = coefs_p @ cov @ coefs_p\n",
    "    sigma_n = coefs_n @ cov @ coefs_n\n",
    "\n",
    "    # print(f\"theta bounds: {ests @ coefs_n}, {ests @ coefs_p}\")\n",
    "    # print(f\"sigma bounds: {sigma_n}, {sigma_p}\")\n",
    "\n",
    "    lb = _safe_norm_ppf(alpha / 2, loc=ests @ coefs_n, scale=np.sqrt(sigma_n))\n",
    "    ub = _safe_norm_ppf(1 - alpha / 2, loc=ests @ coefs_p, scale=np.sqrt(sigma_p))\n",
    "\n",
    "    return (lb, ub)\n",
    "\n",
    "\n",
    "def RV(theta, sigma, nu, cov, alpha):\n",
    "    # The robustness value is the degree of confounding of *both* the treatment and the outcome that still produces an interval\n",
    "    # that excludes zero.\n",
    "\n",
    "    # We're looking for a value of r such that the sensitivity bounds just touch zero\n",
    "\n",
    "    r = 0\n",
    "    r_up = 1\n",
    "    r_down = 0\n",
    "    lb, ub = sensitivity_interval(theta, sigma, nu, cov, alpha, 0, 0, 1)\n",
    "    if lb < 0 and ub > 0:\n",
    "        return 0\n",
    "    \n",
    "    else:\n",
    "        if lb > 0:\n",
    "            target = 0\n",
    "            mult = 1\n",
    "            d = lb\n",
    "        else:\n",
    "            target = 1\n",
    "            mult = -1\n",
    "            d = ub\n",
    "\n",
    "    while abs(d) > 1e-6:\n",
    "        d = mult * sensitivity_interval(theta, sigma, nu, cov, alpha, r, r, 1)[target]\n",
    "        if d > 0:\n",
    "            r_down = r\n",
    "        else:\n",
    "            r_up = r\n",
    "\n",
    "        r = (r_down + r_up) / 2\n",
    "        \n",
    "    return r\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(3)\n",
    "n = 10000\n",
    "alpha = np.random.normal(size=5)\n",
    "X = np.random.normal(size=(n,5), loc=[1,0.5,0,0,0])/3 # closest to center 1, then 2, then 3\n",
    "centers = np.array([[1,0,0,0,0], [0,1,0,0,0], [0,0,1,0,0]]) # trinary treatment\n",
    "# centers = np.array([[1,0,0,0,0], [0,1,0,0,0]]) # uncomment for binary treatment\n",
    "\n",
    "ds = X[:,None,:]-centers[None,:,:]\n",
    "ds = np.einsum(\"nci,nci->nc\", ds, ds)\n",
    "\n",
    "ps_r = np.exp(-ds)\n",
    "ps = ps_r / np.sum(ps_r, axis=1, keepdims=True)\n",
    "\n",
    "T = np.random.default_rng().multinomial(1, ps) @ np.arange(len(centers))\n",
    "\n",
    "Y = np.random.normal(size=n) + 3*(T == 1)*X[:,1] - (T == 2) + 2 * X @ alpha\n",
    "\n",
    "true_theta = np.mean(3*X[:,1]), -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run simple dml and calculate intermediate values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_level = 0.1\n",
    "\n",
    "theta, sigma, nu, sig = dr_sim(LogisticRegression(), LinearRegression(), 2, Y, T, X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate a \"sensitivity interval\". \n",
    "\n",
    "Need to supply \"strength of latent confounder\" as argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i': 0, 'lb': -2.38862155309683, 'ub': 3.424960144685417}\n",
      "{'i': 1, 'lb': -4.2305658461136355, 'ub': 2.24158270347243}\n"
     ]
    }
   ],
   "source": [
    "for i in range(theta.shape[0]):\n",
    "    lb, ub = sensitivity_interval(theta[i], sigma[i], nu[i], sig[i], sig_level, 0.6, 0.6, 1)\n",
    "    print({'i': i, 'lb': lb, 'ub': ub})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Robustness Value. \n",
    "\n",
    "The required strength of a latent confounder in order for the confidence interval to include 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i': 0, 'RV': 0.14217567443847656}\n",
      "{'i': 1, 'RV': 0.24674725532531738}\n"
     ]
    }
   ],
   "source": [
    "for i in range(theta.shape[0]):\n",
    "    rv = RV(theta[i], sigma[i], nu[i], sig[i], sig_level)\n",
    "    print({'i': i, 'RV': rv})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ablations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'i': 1,\n",
       "  'alpha': 0.1,\n",
       "  't_ind': 1,\n",
       "  'sensitivity_interval': (-5.548451847333972, 3.5719993434133452),\n",
       "  'RV': 0.17672276496887207},\n",
       " {'i': 5,\n",
       "  'alpha': 0.1,\n",
       "  't_ind': 1,\n",
       "  'sensitivity_interval': (-4.230585221174876, 2.2303879643275093),\n",
       "  'RV': 0.2489337921142578},\n",
       " {'i': 15,\n",
       "  'alpha': 0.1,\n",
       "  't_ind': 1,\n",
       "  'sensitivity_interval': (-4.2284220073443946, 2.23410292617903),\n",
       "  'RV': 0.24792170524597168},\n",
       " {'i': 30,\n",
       "  'alpha': 0.1,\n",
       "  't_ind': 1,\n",
       "  'sensitivity_interval': (-4.223113136185441, 2.230017984700478),\n",
       "  'RV': 0.24834203720092773}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sig_level = 0.1\n",
    "t_ind = 1\n",
    "\n",
    "results = []\n",
    "for i in [1, 5, 15, 30]:\n",
    "    theta, sigma, nu, sig = dr_sim(LogisticRegression(), LinearRegression(), 2, Y, T, X[:,:i])\n",
    "    result_dict = {\n",
    "        'i': i,\n",
    "        'alpha': sig_level,\n",
    "        't_ind': t_ind,\n",
    "        'sensitivity_interval': sensitivity_interval(theta[t_ind], sigma[t_ind], nu[t_ind], sig[t_ind], sig_level, 0.6, 0.6, 1),\n",
    "        'RV': RV(theta[t_ind], sigma[t_ind], nu[t_ind], sig[t_ind], sig_level)\n",
    "    }\n",
    "    results.append(result_dict)\n",
    "                   \n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>i</th>\n",
       "      <th>alpha</th>\n",
       "      <th>t_ind</th>\n",
       "      <th>sensitivity_interval</th>\n",
       "      <th>RV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>(-5.548451847333972, 3.5719993434133452)</td>\n",
       "      <td>0.176723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>(-4.230585221174876, 2.2303879643275093)</td>\n",
       "      <td>0.248934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>(-4.2284220073443946, 2.23410292617903)</td>\n",
       "      <td>0.247922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>(-4.223113136185441, 2.230017984700478)</td>\n",
       "      <td>0.248342</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    i  alpha  t_ind                      sensitivity_interval        RV\n",
       "0   1    0.1      1  (-5.548451847333972, 3.5719993434133452)  0.176723\n",
       "1   5    0.1      1  (-4.230585221174876, 2.2303879643275093)  0.248934\n",
       "2  15    0.1      1   (-4.2284220073443946, 2.23410292617903)  0.247922\n",
       "3  30    0.1      1   (-4.223113136185441, 2.230017984700478)  0.248342"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    pd.DataFrame(results)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DoubleML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import doubleml as dml\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Y</th>\n",
       "      <th>T</th>\n",
       "      <th>X0</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.607903</td>\n",
       "      <td>0</td>\n",
       "      <td>0.215080</td>\n",
       "      <td>0.139086</td>\n",
       "      <td>-0.209000</td>\n",
       "      <td>-0.014606</td>\n",
       "      <td>-0.159073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.808441</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.104622</td>\n",
       "      <td>0.461541</td>\n",
       "      <td>0.293773</td>\n",
       "      <td>0.569858</td>\n",
       "      <td>0.016678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.676531</td>\n",
       "      <td>1</td>\n",
       "      <td>0.198441</td>\n",
       "      <td>-0.015120</td>\n",
       "      <td>-0.515492</td>\n",
       "      <td>0.327456</td>\n",
       "      <td>-0.367023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.235249</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.061682</td>\n",
       "      <td>0.098117</td>\n",
       "      <td>0.495383</td>\n",
       "      <td>0.078905</td>\n",
       "      <td>-0.341262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.764248</td>\n",
       "      <td>1</td>\n",
       "      <td>0.095669</td>\n",
       "      <td>0.375082</td>\n",
       "      <td>-0.053504</td>\n",
       "      <td>-0.256279</td>\n",
       "      <td>-0.076677</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Y  T        X0        X1        X2        X3        X4\n",
       "0  0.607903  0  0.215080  0.139086 -0.209000 -0.014606 -0.159073\n",
       "1 -1.808441  2 -0.104622  0.461541  0.293773  0.569858  0.016678\n",
       "2 -1.676531  1  0.198441 -0.015120 -0.515492  0.327456 -0.367023\n",
       "3 -1.235249  2 -0.061682  0.098117  0.495383  0.078905 -0.341262\n",
       "4  2.764248  1  0.095669  0.375082 -0.053504 -0.256279 -0.076677"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([\n",
    "    pd.DataFrame(Y).squeeze().to_frame('Y'),\n",
    "    pd.DataFrame(T).squeeze().to_frame('T'),\n",
    "    pd.DataFrame(X).add_prefix('X'),\n",
    "], axis=1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<doubleml.double_ml_data.DoubleMLData at 0x1f0fa322b30>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dml_data = dml.DoubleMLData(df, 'Y', 'T')\n",
    "dml_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================== DoubleMLPLR Object ==================\n",
      "\n",
      "------------------ Data summary      ------------------\n",
      "Outcome variable: Y\n",
      "Treatment variable(s): ['T']\n",
      "Covariates: ['X0', 'X1', 'X2', 'X3', 'X4']\n",
      "Instrument variable(s): None\n",
      "No. Observations: 10000\n",
      "\n",
      "------------------ Score & algorithm ------------------\n",
      "Score function: partialling out\n",
      "\n",
      "------------------ Machine learner   ------------------\n",
      "Learner ml_l: LinearRegression()\n",
      "Learner ml_m: LinearRegression()\n",
      "Out-of-sample Performance:\n",
      "Regression:\n",
      "Learner ml_l RMSE: [[1.24933754]]\n",
      "Learner ml_m RMSE: [[0.75572709]]\n",
      "\n",
      "------------------ Resampling        ------------------\n",
      "No. folds: 2\n",
      "No. repeated sample splits: 1\n",
      "\n",
      "------------------ Fit summary       ------------------\n",
      "       coef   std err          t          P>|t|     2.5 %    97.5 %\n",
      "T -0.397119  0.015463 -25.682281  1.844035e-145 -0.427425 -0.366812\n"
     ]
    }
   ],
   "source": [
    "dml_obj = dml.DoubleMLPLR(dml_data,\n",
    "                          ml_l=LinearRegression(),\n",
    "                          ml_m=LinearRegression(),\n",
    "                          n_folds=2,\n",
    "                          score='partialling out',)\n",
    "dml_obj.fit()\n",
    "print(dml_obj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================== Sensitivity Analysis ==================\n",
      "\n",
      "------------------ Scenario          ------------------\n",
      "Significance Level: level=0.95\n",
      "Sensitivity parameters: cf_y=0.03; cf_d=0.03, rho=1.0\n",
      "\n",
      "------------------ Bounds with CI    ------------------\n",
      "   CI lower  theta lower     theta  theta upper  CI upper\n",
      "0 -0.471507       -0.446 -0.397119    -0.348237 -0.322857\n",
      "\n",
      "------------------ Robustness Values ------------------\n",
      "   H_0     RV (%)    RVa (%)\n",
      "0  0.0  21.873191  20.629383\n"
     ]
    }
   ],
   "source": [
    "dml_obj.sensitivity_analysis(cf_y=0.03, cf_d=0.03, rho=1.)\n",
    "print(dml_obj.sensitivity_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       cf_y      cf_d  rho  delta_theta\n",
      "T  0.019825  0.000095 -1.0      -0.0022\n"
     ]
    }
   ],
   "source": [
    "sens_benchmark = dml_obj.sensitivity_benchmark(benchmarking_set=[\"X4\"])\n",
    "print(sens_benchmark)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DoubleML confounded synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from doubleml.datasets import make_confounded_plr_data, make_confounded_irm_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fabiovera\\AppData\\Local\\anaconda3\\envs\\dev_env2\\lib\\site-packages\\doubleml\\datasets.py:1050: UserWarning: Propensity score is close to 0 or 1. Trimming is at 0.01 and 0.99 is applied\n",
      "  warnings.warn(f'Propensity score is close to 0 or 1. '\n"
     ]
    }
   ],
   "source": [
    "cf_y = 0.1\n",
    "cf_d = 0.1\n",
    "theta = 5.0\n",
    "dpg_dict = make_confounded_irm_data(n_obs=10000, cf_y=cf_y, cf_d=cf_d, theta=theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cols = [f'X{i + 1}' for i in np.arange(dpg_dict['x'].shape[1])]\n",
    "df = pd.DataFrame(np.column_stack((dpg_dict['x'], dpg_dict['y'], dpg_dict['d'])), columns=x_cols + ['y', 'd'])\n",
    "dml_data = dml.DoubleMLData(df, 'y', 'd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================== DoubleMLIRM Object ==================\n",
      "\n",
      "------------------ Data summary      ------------------\n",
      "Outcome variable: y\n",
      "Treatment variable(s): ['d']\n",
      "Covariates: ['X1', 'X2', 'X3', 'X4', 'X5']\n",
      "Instrument variable(s): None\n",
      "No. Observations: 10000\n",
      "\n",
      "------------------ Score & algorithm ------------------\n",
      "Score function: ATE\n",
      "\n",
      "------------------ Machine learner   ------------------\n",
      "Learner ml_g: RandomForestRegressor()\n",
      "Learner ml_m: RandomForestClassifier()\n",
      "Out-of-sample Performance:\n",
      "Regression:\n",
      "Learner ml_g0 RMSE: [[1.08528722]]\n",
      "Learner ml_g1 RMSE: [[1.14348731]]\n",
      "Classification:\n",
      "Learner ml_m Log Loss: [[0.67196054]]\n",
      "\n",
      "------------------ Resampling        ------------------\n",
      "No. folds: 2\n",
      "No. repeated sample splits: 1\n",
      "\n",
      "------------------ Fit summary       ------------------\n",
      "       coef   std err          t  P>|t|   2.5 %   97.5 %\n",
      "d  5.145625  0.062208  82.716619    0.0  5.0237  5.26755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fabiovera\\AppData\\Local\\anaconda3\\envs\\dev_env2\\lib\\site-packages\\doubleml\\utils\\_checks.py:205: UserWarning: Propensity predictions from learner RandomForestClassifier() for ml_m are close to zero or one (eps=1e-12).\n",
      "  warnings.warn(f'Propensity predictions from learner {str(learner)} for'\n"
     ]
    }
   ],
   "source": [
    "dml_obj = dml.DoubleMLIRM(dml_data,\n",
    "                          ml_g=RandomForestRegressor(),\n",
    "                          ml_m=RandomForestClassifier(),\n",
    "                          n_folds=2,\n",
    "                          score='ATE',)\n",
    "dml_obj.fit()\n",
    "print(dml_obj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================== Sensitivity Analysis ==================\n",
      "\n",
      "------------------ Scenario          ------------------\n",
      "Significance Level: level=0.95\n",
      "Sensitivity parameters: cf_y=0.1; cf_d=0.1, rho=1.0\n",
      "\n",
      "------------------ Bounds with CI    ------------------\n",
      "   CI lower  theta lower     theta  theta upper  CI upper\n",
      "0  4.858139     4.961768  5.145625     5.329482  5.466168\n",
      "\n",
      "------------------ Robustness Values ------------------\n",
      "   H_0     RV (%)    RVa (%)\n",
      "0  0.0  90.573845  84.951031\n"
     ]
    }
   ],
   "source": [
    "dml_obj.sensitivity_analysis(cf_y=0.1, cf_d=0.1, rho=1.)\n",
    "print(dml_obj.sensitivity_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 0.05,\n",
       " 'sensitivity_interval': (2.567683475272766, 7.648542718465433),\n",
       " 'RV': 0.8172124624252319}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sig_level=0.05\n",
    "t_ind = 0\n",
    "\n",
    "theta, sigma, nu, sig = dr_sim(\n",
    "    RandomForestClassifier(), \n",
    "    RandomForestRegressor(), \n",
    "    2, \n",
    "    dpg_dict['y'],\n",
    "    dpg_dict['d'].astype(int),\n",
    "    dpg_dict['x']\n",
    ")\n",
    "\n",
    "result_dict = {\n",
    "    'alpha': sig_level,\n",
    "    'sensitivity_interval': sensitivity_interval(theta[t_ind], sigma[t_ind], nu[t_ind], sig[t_ind], sig_level, 0.6, 0.6, 1),\n",
    "    'RV': RV(theta[t_ind], sigma[t_ind], nu[t_ind], sig[t_ind], sig_level)\n",
    "}\n",
    "result_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 401k data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "dml_data = dml.datasets.fetch_401K()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model selection for propensity model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier selected with score: 0.6435\n",
      "401k data treatment model selection:\n",
      "Best classifier: RandomForestClassifier(max_depth=10, n_estimators=200, random_state=42)\n",
      "Best score: 0.6435\n",
      "best rf score: 0.6435\n",
      "best lr score: 0.6578\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define parameter grid for model selection\n",
    "# Define parameter grid for model selection\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100]  # For LogisticRegression\n",
    "}\n",
    "\n",
    "# First find the best LogisticRegression model\n",
    "lr_grid_search = GridSearchCV(\n",
    "    estimator=LogisticRegression(max_iter=1000),\n",
    "    param_grid=param_grid,\n",
    "    cv=3,\n",
    "    scoring='accuracy'\n",
    ")\n",
    "lr_grid_search.fit(dml_data.x, dml_data.d.astype(int))\n",
    "best_lr = lr_grid_search.best_estimator_\n",
    "best_lr_score = lr_grid_search.best_score_\n",
    "\n",
    "# Find the best RandomForestClassifier\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20]\n",
    "}\n",
    "rf_grid_search = GridSearchCV(\n",
    "    estimator=RandomForestClassifier(random_state=42),\n",
    "    param_grid=rf_param_grid,\n",
    "    cv=3,\n",
    "    scoring='accuracy'\n",
    ")\n",
    "rf_grid_search.fit(dml_data.x, dml_data.d.astype(int))\n",
    "best_rf = rf_grid_search.best_estimator_\n",
    "best_rf_score = rf_grid_search.best_score_\n",
    "\n",
    "# Choose the overall best model\n",
    "if best_lr_score >= best_rf_score:\n",
    "    grid_search = lr_grid_search\n",
    "    print(f\"LogisticRegression selected with score: {best_lr_score:.4f}\")\n",
    "else:\n",
    "    grid_search = rf_grid_search\n",
    "print(f\"RandomForestClassifier selected with score: {best_rf_score:.4f}\")\n",
    "\n",
    "# Use the best estimator from grid search\n",
    "best_classifier = grid_search.best_estimator_\n",
    "print('401k data treatment model selection:')\n",
    "print(f\"Best classifier: {best_classifier}\")\n",
    "print(f\"Best score: {grid_search.best_score_:.4f}\")\n",
    "print(f\"best rf score: {best_rf_score:.4f}\")\n",
    "print(f\"best lr score: {best_lr_score:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================== DoubleMLIRM Object ==================\n",
      "\n",
      "------------------ Data summary      ------------------\n",
      "Outcome variable: inuidur1\n",
      "Treatment variable(s): ['tg']\n",
      "Covariates: ['female', 'black', 'othrace', 'dep1', 'dep2', 'q2', 'q3', 'q4', 'q5', 'q6', 'agelt35', 'agegt54', 'durable', 'lusd', 'husd']\n",
      "Instrument variable(s): None\n",
      "No. Observations: 5099\n",
      "\n",
      "------------------ Score & algorithm ------------------\n",
      "Score function: ATE\n",
      "\n",
      "------------------ Machine learner   ------------------\n",
      "Learner ml_g: RandomForestRegressor()\n",
      "Learner ml_m: RandomForestClassifier(max_depth=10, n_estimators=200, random_state=42)\n",
      "Out-of-sample Performance:\n",
      "Regression:\n",
      "Learner ml_g0 RMSE: [[1.27853275]]\n",
      "Learner ml_g1 RMSE: [[1.29332229]]\n",
      "Classification:\n",
      "Learner ml_m Log Loss: [[0.6649765]]\n",
      "\n",
      "------------------ Resampling        ------------------\n",
      "No. folds: 5\n",
      "No. repeated sample splits: 1\n",
      "\n",
      "------------------ Fit summary       ------------------\n",
      "        coef   std err         t     P>|t|     2.5 %    97.5 %\n",
      "tg -0.074699  0.043994 -1.697941  0.089519 -0.160925  0.011527\n"
     ]
    }
   ],
   "source": [
    "dml_obj = dml.DoubleMLIRM(dml_data,\n",
    "                          ml_g=RandomForestRegressor(),\n",
    "                          ml_m=best_classifier,\n",
    "                          n_folds=5,\n",
    "                          score='ATE')\n",
    "dml_obj.fit()\n",
    "print(dml_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================== Sensitivity Analysis ==================\n",
      "\n",
      "------------------ Scenario          ------------------\n",
      "Significance Level: level=0.95\n",
      "Sensitivity parameters: cf_y=0.04; cf_d=0.04, rho=1.0\n",
      "\n",
      "------------------ Bounds with CI    ------------------\n",
      "   CI lower  theta lower     theta  theta upper  CI upper\n",
      "0 -0.246676    -0.174045 -0.074699     0.024648  0.096968\n",
      "\n",
      "------------------ Robustness Values ------------------\n",
      "   H_0    RV (%)  RVa (%)\n",
      "0  0.0  3.022945  0.09602\n"
     ]
    }
   ],
   "source": [
    "dml_obj.sensitivity_analysis(cf_y=0.04, cf_d=0.04, rho=1.)\n",
    "print(dml_obj.sensitivity_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 0.05,\n",
       " 'sensitivity_interval': (-0.15765123322817087, 0.01212238369880464),\n",
       " 'RV': 0}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sig_level=0.05\n",
    "t_ind = 0\n",
    "\n",
    "theta, sigma, nu, sig = dr_sim(\n",
    "    best_classifier, \n",
    "    RandomForestRegressor(), \n",
    "    5, \n",
    "    dml_data.y,\n",
    "    dml_data.d.astype(int),\n",
    "    dml_data.x\n",
    ")\n",
    "\n",
    "result_dict = {\n",
    "    'alpha': sig_level,\n",
    "    'sensitivity_interval': sensitivity_interval(theta[t_ind], sigma[t_ind], nu[t_ind], sig[t_ind], sig_level, 0.00001, 0.0001, 1),\n",
    "    'RV': RV(theta[t_ind], sigma[t_ind], nu[t_ind], sig[t_ind], sig_level)\n",
    "}\n",
    "result_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bonus data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "dml_data = dml.datasets.fetch_bonus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model selection for propensity model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier selected with score: -0.6624\n",
      "bonus data treatment model selection:\n",
      "Best classifier: RandomForestClassifier(max_depth=10, n_estimators=200, random_state=42)\n",
      "Best score: -0.6624\n",
      "best rf score: -0.6624\n",
      "best lr score: -0.6442\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define parameter grid for model selection\n",
    "# Define parameter grid for model selection\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100]  # For LogisticRegression\n",
    "}\n",
    "\n",
    "# First find the best LogisticRegression model\n",
    "lr_grid_search = GridSearchCV(\n",
    "    estimator=LogisticRegression(max_iter=1000),\n",
    "    param_grid=param_grid,\n",
    "    cv=3,\n",
    "    scoring='neg_log_loss'\n",
    ")\n",
    "lr_grid_search.fit(dml_data.x, dml_data.d.astype(int))\n",
    "best_lr = lr_grid_search.best_estimator_\n",
    "best_lr_score = lr_grid_search.best_score_\n",
    "\n",
    "# Find the best RandomForestClassifier\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20]\n",
    "}\n",
    "rf_grid_search = GridSearchCV(\n",
    "    estimator=RandomForestClassifier(random_state=42),\n",
    "    param_grid=rf_param_grid,\n",
    "    cv=3,\n",
    "    scoring='neg_log_loss'\n",
    ")\n",
    "rf_grid_search.fit(dml_data.x, dml_data.d.astype(int))\n",
    "best_rf = rf_grid_search.best_estimator_\n",
    "best_rf_score = rf_grid_search.best_score_\n",
    "\n",
    "# Choose the overall best model\n",
    "# if best_lr_score >= best_rf_score:\n",
    "#     grid_search = lr_grid_search\n",
    "#     print(f\"LogisticRegression selected with score: {best_lr_score:.4f}\")\n",
    "# else:\n",
    "grid_search = rf_grid_search\n",
    "print(f\"RandomForestClassifier selected with score: {best_rf_score:.4f}\")\n",
    "\n",
    "# Use the best estimator from grid search\n",
    "best_classifier = grid_search.best_estimator_\n",
    "print('bonus data treatment model selection:')\n",
    "print(f\"Best classifier: {best_classifier}\")\n",
    "print(f\"Best score: {grid_search.best_score_:.4f}\")\n",
    "print(f\"best rf score: {best_rf_score:.4f}\")\n",
    "print(f\"best lr score: {best_lr_score:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================== DoubleMLIRM Object ==================\n",
      "\n",
      "------------------ Data summary      ------------------\n",
      "Outcome variable: inuidur1\n",
      "Treatment variable(s): ['tg']\n",
      "Covariates: ['female', 'black', 'othrace', 'dep1', 'dep2', 'q2', 'q3', 'q4', 'q5', 'q6', 'agelt35', 'agegt54', 'durable', 'lusd', 'husd']\n",
      "Instrument variable(s): None\n",
      "No. Observations: 5099\n",
      "\n",
      "------------------ Score & algorithm ------------------\n",
      "Score function: ATE\n",
      "\n",
      "------------------ Machine learner   ------------------\n",
      "Learner ml_g: RandomForestRegressor()\n",
      "Learner ml_m: RandomForestClassifier(max_depth=10, n_estimators=200, random_state=42)\n",
      "Out-of-sample Performance:\n",
      "Regression:\n",
      "Learner ml_g0 RMSE: [[1.28597712]]\n",
      "Learner ml_g1 RMSE: [[1.31606153]]\n",
      "Classification:\n",
      "Learner ml_m Log Loss: [[0.66950577]]\n",
      "\n",
      "------------------ Resampling        ------------------\n",
      "No. folds: 2\n",
      "No. repeated sample splits: 1\n",
      "\n",
      "------------------ Fit summary       ------------------\n",
      "        coef   std err        t     P>|t|     2.5 %    97.5 %\n",
      "tg -0.064992  0.045702 -1.42208  0.155003 -0.154566  0.024582\n"
     ]
    }
   ],
   "source": [
    "dml_obj = dml.DoubleMLIRM(dml_data,\n",
    "                          ml_g=RandomForestRegressor(),\n",
    "                          ml_m=best_classifier,\n",
    "                          n_folds=2,\n",
    "                          score='ATE',)\n",
    "dml_obj.fit()\n",
    "print(dml_obj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================== Sensitivity Analysis ==================\n",
      "\n",
      "------------------ Scenario          ------------------\n",
      "Significance Level: level=0.95\n",
      "Sensitivity parameters: cf_y=0.04; cf_d=0.04, rho=1.0\n",
      "\n",
      "------------------ Bounds with CI    ------------------\n",
      "   CI lower  theta lower     theta  theta upper  CI upper\n",
      "0 -0.232833    -0.157464 -0.064992     0.027481  0.103315\n",
      "\n",
      "------------------ Robustness Values ------------------\n",
      "   H_0    RV (%)   RVa (%)\n",
      "0  0.0  2.828461  0.000603\n"
     ]
    }
   ],
   "source": [
    "dml_obj.sensitivity_analysis(cf_y=0.04, cf_d=0.04, rho=1.)\n",
    "print(dml_obj.sensitivity_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 0.05,\n",
       " 'sensitivity_interval': (-0.29151616548820364, 0.1365988298703495),\n",
       " 'RV': 0}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sig_level=0.05\n",
    "t_ind = 0\n",
    "\n",
    "theta, sigma, nu, sig = dr_sim(\n",
    "    best_classifier, \n",
    "    RandomForestRegressor(), \n",
    "    5, \n",
    "    dml_data.y,\n",
    "    dml_data.d.astype(int),\n",
    "    dml_data.x\n",
    ")\n",
    "\n",
    "result_dict = {\n",
    "    'alpha': sig_level,\n",
    "    'sensitivity_interval': sensitivity_interval(theta[t_ind], sigma[t_ind], nu[t_ind], sig[t_ind], sig_level, 0.05, 0.05, 1),\n",
    "    'RV': RV(theta[t_ind], sigma[t_ind], nu[t_ind], sig[t_ind], sig_level)\n",
    "}\n",
    "result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev_env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
